{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b4f80d",
   "metadata": {},
   "source": [
    "# Imports and Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df18b5",
   "metadata": {},
   "source": [
    "## Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e541682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import uuid\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966058c",
   "metadata": {},
   "source": [
    "## Initialise Mediapipe variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising MediaPipe's holistic model and Drawing Utilites\n",
    "\n",
    "#Allows for simultaneous detection of body, hand and facial landmarks\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "#Helps to draw detection landmarks onto the image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d388d2",
   "metadata": {},
   "source": [
    "# Function definitions for Landmark Detection and Keypoint Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for detecting landmarks using MediaPipe and OpenCV\n",
    "def landmarks_detection(image, model):\n",
    "    \n",
    "    #Converting image from BGR to RGB since OpenCV uses BGR as default while MediaPipe uses RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    \n",
    "    #Turning off writeable feature of Numpy array as MediaPipe will crash when trying to process writeable arrays\n",
    "    image.flags.writeable = False   \n",
    "    \n",
    "    #Process the image to get the detection results\n",
    "    detection_results = model.process(image)   \n",
    "    \n",
    "    #Turning the writeability of Numpy array back on\n",
    "    image.flags.writeable = True \n",
    "    \n",
    "    #Turning the image back to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    \n",
    "    #Returning image and the detection results\n",
    "    return image, detection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Define drawing specs for each landmark type\n",
    "    drawing_specs = {\n",
    "        'face': mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "        'pose': mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "        'left_hand': mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "        'right_hand': mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4)\n",
    "    }\n",
    "\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             drawing_specs['face'], drawing_specs['face'])\n",
    "                             \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             drawing_specs['pose'], drawing_specs['pose'])\n",
    "\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             drawing_specs['left_hand'], drawing_specs['left_hand'])\n",
    "                             \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             drawing_specs['right_hand'], drawing_specs['right_hand']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ab105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to extract x, y, z, visibility from each landmarks\n",
    "def get_landmarks(landmarks, num_landmarks, dimensions=4):\n",
    "    \n",
    "    #If landmarks is detected\n",
    "    if landmarks:\n",
    "        #Returning a flattened array containing x, y, z, and visibility values of each landmark based on if visibility is present or not\n",
    "        return np.array([[landmark.x, landmark.y, landmark.z] + ([landmark.visibility] if dimensions == 4 else []) for landmark in landmarks.landmark]).flatten()\n",
    "    \n",
    "    #If no landmarks are detected returning a zero filled numpy array with size equal to the number of expected landmarks multiplied by the number of dimensions of each landmark\n",
    "    return np.zeros(num_landmarks * dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5be918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \n",
    "    #Extracting pose keypoints\n",
    "    # 33 landmarks are expected for pose, and each landmark has 4 values (x, y, z, visibility)\n",
    "    pose = get_landmarks(results.pose_landmarks, 33, 4)\n",
    "    \n",
    "    #Extracting face keypoints\n",
    "    #468 landmarks are expected for face and each landmark has 3 values (x, y, z)\n",
    "    face = get_landmarks(results.face_landmarks, 468, 3)\n",
    "    \n",
    "    #Extracting left hand keypoints\n",
    "    #21 landmarks are expected for face and each landmark has 3 values (x, y, z)\n",
    "    lh = get_landmarks(results.left_hand_landmarks, 21, 3)\n",
    "    \n",
    "    #Extracting right hand keypoints\n",
    "    #21 landmarks are expected for face and each landmark has 3 values (x, y, z)\n",
    "    rh = get_landmarks(results.right_hand_landmarks, 21, 3)\n",
    "\n",
    "    #Concatenating all the keypoints into a single numpy array and returning it\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfce32",
   "metadata": {},
   "source": [
    "# Setting up Video Capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17886c22",
   "metadata": {},
   "source": [
    "## Make Detections and Draw Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5062f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the webcam feed with OpenCV's VideoCapture object\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_AVFOUNDATION)\n",
    "\n",
    "#Setting mediapipe's holistic model for simultaneous body, face and hand landmark detection \n",
    "# The minimum detection and tracking confidence parameters are set to 0.5 to begin tracking.\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #Continues as long as the webcam feed is active\n",
    "    while cap.isOpened():\n",
    "\n",
    "        #Reads a frame from the webcam \n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #Making detections on the frame using 'landmarks_detection' function that uses holistic model to process the frame\n",
    "        image, results = landmarks_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        #Drawing the detected landmarks on the image using the draw_styled_landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        #Displaying the processed fram with the detected landmarks drawn on it in a new window\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        #The 'waitKey' function listens for any keyboard event for the time specified in milliseconds. If 'q' is pressed, the loop is broken.\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    #Release the webcam once finished\n",
    "    cap.release()\n",
    "    \n",
    "    #Close all OpenCV windows once finished\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752eca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_styled_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecbf99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71defb",
   "metadata": {},
   "source": [
    "# Data Collection for Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69a068",
   "metadata": {},
   "source": [
    "## Define Actions and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path for the directory where the exported data, in the form of numpy arrays, will be stored\n",
    "DATA_PATH = os.path.join('Data') \n",
    "\n",
    "#Defint the list of actions to be detected by the model\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "#Specify the number of sequences to be captured for each action\n",
    "#This number refers to the number of separate instances or examples of each action that we want to capture\n",
    "sequence_count = 30\n",
    "\n",
    "#Specify the length of each sequence captured\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf067a",
   "metadata": {},
   "source": [
    "## Creating necessary Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function checks if a given directory exists, and if it doesn't, it creates it\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function calculates the number for the new directory to be created\n",
    "def get_new_directory_number(parent_directory):\n",
    "    \n",
    "    #By checking the existing directories, converting their names to integers, and then choosing the maximum.\n",
    "    #The new directory number is then one more than the maximum.\n",
    "    if not os.listdir(parent_directory):\n",
    "        return 0\n",
    "    else:\n",
    "        return np.max(np.array(os.listdir(parent_directory)).astype(int)) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function creates a specified number of directories for sequences within a parent directory\n",
    "def create_sequence_directories(parent_directory, sequence_count):\n",
    "    \n",
    "    # The names of these directories are determined by the get_new_directory_number function\n",
    "    start_sequence = get_new_directory_number(parent_directory)\n",
    "    \n",
    "    for sequence in range(start_sequence, start_sequence + sequence_count + 1):\n",
    "        create_directory(os.path.join(parent_directory, str(sequence)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops over the defined actions\n",
    "for action in actions:\n",
    "    \n",
    "    #For each action, a directory is created in the defined export path\n",
    "    action_directory = os.path.join(DATA_PATH, action)\n",
    "    \n",
    "    create_directory(action_directory)\n",
    "    \n",
    "    #A number of sequence directories are created within the action directory\n",
    "    create_sequence_directories(action_directory, sequence_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c4fa4",
   "metadata": {},
   "source": [
    "# Collect keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the webcam feed with OpenCV's VideoCapture object\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "#Setting mediapipe's holistic model for simultaneous body, face and hand landmark detection \n",
    "# The minimum detection and tracking confidence parameters are set to 0.5 to begin tracking\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic_model:\n",
    "    \n",
    "    #Loops over each action to be detected\n",
    "    for action in actions:\n",
    "        \n",
    "        #Loops over the number of sequences for each action\n",
    "        for sequence_index in range(sequence_count + 1):\n",
    "            \n",
    "            #Loops over each frame of a sequence\n",
    "            for frame_index in range(sequence_length + 1):\n",
    "\n",
    "                #Capture a frame from the video stream\n",
    "                ret, frame = video_capture.read()\n",
    "\n",
    "                #Process the frame to detect and get the landmarks\n",
    "                image, results = landmarks_detection(frame, holistic_model)\n",
    "\n",
    "                #Visualize the landmarks on the image\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                #If this is the first frame of the sequence, display a message and wait before starting the collection\n",
    "                if frame_index == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence_index), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence_index), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                #Extracting the keypoints from the detection results\n",
    "                keypoints = extract_keypoints(results)\n",
    "                \n",
    "                #Defining the path for saving the keypoints into a numpy file\n",
    "                keypoints_save_path = os.path.join(DATA_PATH, action, str(sequence_index), str(frame_index))\n",
    "                \n",
    "                #Saving the keypoints into a numpy file\n",
    "                np.save(keypoints_save_path, keypoints)\n",
    "\n",
    "                #Exiting the program if 'q' is pressed\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    #Releasing the video capture object and destroy all OpenCV windows when done\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300dfd5",
   "metadata": {},
   "source": [
    "# Preprocessing and Data Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dictionary to map actions to integers\n",
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb61e87",
   "metadata": {},
   "source": [
    "## Load and Organise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load data\n",
    "def load_data(DATA_PATH, actions, sequence_length, label_map):\n",
    "    \n",
    "    #Initialise lists for sequences and labels\n",
    "    sequences, labels = [], []\n",
    "\n",
    "    #Load sequences and labels\n",
    "    for action in actions:\n",
    "        for sequence in os.listdir(os.path.join(DATA_PATH, action)):\n",
    "            \n",
    "            #Skip files that are not directories\n",
    "            if not os.path.isdir(os.path.join(DATA_PATH, action, sequence)):\n",
    "                continue\n",
    "            \n",
    "            window = []\n",
    "            \n",
    "            for frame_num in range(sequence_length):\n",
    "                frame_path = os.path.join(DATA_PATH, action, sequence, f\"{frame_num}.npy\")\n",
    "                frame_keypoints = np.load(frame_path)\n",
    "                window.append(frame_keypoints)\n",
    "            \n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "\n",
    "    return np.array(sequences), to_categorical(labels).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abea7ca",
   "metadata": {},
   "source": [
    "# Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, actions):\n",
    "    #Initialise the model\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add LSTM layers\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "    #Add Dense layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(len(actions), activation='softmax'))\n",
    "\n",
    "    #Compiles the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc007c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the  data\n",
    "X, y = load_data(DATA_PATH, actions, sequence_length, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce730b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates model\n",
    "model = create_model((sequence_length, X.shape[2]), actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define callbacks\n",
    "log_dir = 'Logs'\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cdefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains the model\n",
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35089be1",
   "metadata": {},
   "source": [
    "# Saving the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357b67f",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes predictions with the model\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297aab8",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38aaa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the predictions and true values to lists\n",
    "predicted_labels = np.argmax(predictions, axis=1).tolist()\n",
    "true_labels = np.argmax(y_test, axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e977f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the confusion matrix and accuracy score\n",
    "print(multilabel_confusion_matrix(true_labels, predicted_labels))\n",
    "print(accuracy_score(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d655620",
   "metadata": {},
   "source": [
    "# Using the Model for Real - Time Action Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17736185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines a set of colors for the visualization\n",
    "color_palette = [(245,117,16), (117,245,16), (16,117,245)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af326228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function visualises the prediction probabilities on the output frame.\n",
    "def visualise_probabilities(predictions, actions, input_frame, color_palette):\n",
    "    \n",
    "    #Creatse a copy of the input frame to draw on\n",
    "    output_frame = input_frame.copy()\n",
    "\n",
    "    #Loops over the prediction probabilities and draw a filled rectangle for each action\n",
    "    for num, prob in enumerate(predictions):\n",
    "        \n",
    "        #Calculates the width of the rectangle proportional to the prediction probability\n",
    "        rect_width = int(prob * 100)\n",
    "\n",
    "        #Draws the filled rectangle on the output frame\n",
    "        cv2.rectangle(output_frame, (0, 60+num*40), (rect_width, 90+num*40), color_palette[num], -1)\n",
    "\n",
    "        #Adds the action's name text over the rectangle\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    #Returns the annotated output frame\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae492a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines sequence, sentence and predictions as empty lists\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "\n",
    "#Sets a threshold for prediction probability\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b83586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise the camera feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Initialises the mediapipe holistic model with specified confidence parameters\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    #Starts a loop that continues as long as the camera is opened\n",
    "    while cap.isOpened():\n",
    "\n",
    "        #Captures each frame from the camera feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #Uses the landmarks_detection function to detect landmarks on the frame\n",
    "        image, results = landmarks_detection(frame, holistic)\n",
    "        \n",
    "        #Draws the detected landmarks on the frame\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        #Extracts keypoints from the detected landmarks\n",
    "        keypoints = extract_keypoints(results)\n",
    "\n",
    "        #Appends the keypoints to the sequence\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]  # keep the last 30 sets of keypoints\n",
    "        \n",
    "        #If the sequence length has reached 30, predict the action\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))  # store the predicted action\n",
    "            \n",
    "            #If the last 10 predictions are the same and above the threshold, add the action to the sentence\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    \n",
    "                    #Only add new actions to the sentence\n",
    "                    if len(sentence) == 0 or actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            #Limits the sentence length to the last 5 actions\n",
    "            sentence = sentence[-5:]\n",
    "\n",
    "            #Visualise the prediction probabilities on the frame\n",
    "            image = visualise_probabilities(res, actions, image, color_palette)\n",
    "        \n",
    "        #Display the sentence on the frame\n",
    "        cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        #Shows the frame on the screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        #Breaks the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    #Releases the camera and close all windows when done\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea459ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7cf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562bdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31513b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14bd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604cd784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b47f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960faf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c9bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
